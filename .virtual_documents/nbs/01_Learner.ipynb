get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")


#| hide
#| default_exp learner


from fastcore.test import *


#| export
import torch
import gpytorch
from gpfa_imputation.core import *
from fastcore.foundation import *
from fastprogress.fastprogress import progress_bar


class GPFALearner():
    def __init__(self, X):
        self.prepare_X(X)
        self.prepare_time(X)
        
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()
        latent_kernel = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
        self.model = GPFA(self.T, self.X, self.likelihood, self.n_features, latent_kernel)
        
    def prepare_X(self, X):
        # normalize X and store mean to recostruct dataset
        self.x_mean = X.mean(axis=0)
        self.x_std = X.std(axis=0)
        
        # flatten Matrix to vector
        self.X = ((X - self.x_mean) / self.x_std).reshape(-1) 
        self.n_features = X.shape[1]
        
    def prepare_time(self, X):
        self.T = torch.arange(X.shape[0])
        
    
    def train(self, n_iter=100, lr=0.1):
        # need to enable training mode
        self.model.train()
        self.likelihood.train()
        
        # Use the adam optimizer
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr) 
        
        self.losses = torch.empty(n_iter)
        # "Loss" for GPs - the marginal log likelihood
        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)

        for i in progress_bar(range(n_iter)):
            # Zero gradients from previous iteration
            optimizer.zero_grad()
            # Output from model
            output = self.model(self.T)
            # Calc loss and backprop gradients
            loss = -mll(output, self.X)
            self.losses[i] = loss.detach()
            loss.backward()
            self.printer()
            
            optimizer.step()
    
    def printer(self):
        pass
    
    
        


# test data
T = torch.arange(0,6)

X = torch.vstack([torch.arange(0,3, dtype=torch.float32) + 2* i for i in T]) 


X


learner = GPFALearner(X)


test_eq(T, learner.T)


test_eq(learner.n_features, 3)


learner.X


learner.train()


learner.losses


#| hide
from nbdev import nbdev_export
nbdev_export()
