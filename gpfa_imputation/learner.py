# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_Learner.ipynb.

# %% auto 0
__all__ = ['NormParam', 'Normalizer', 'GPFALearner']

# %% ../nbs/01_Learner.ipynb 4
import torch
from torch import Tensor
from torch.distributions import MultivariateNormal 

import gpytorch
from .gpfa import *
from collections import namedtuple
import math

from fastcore.foundation import *
from fastprogress.fastprogress import progress_bar, master_bar
from fastcore.foundation import patch

# %% ../nbs/01_Learner.ipynb 10
class Normalizer:
    def __init__(self,
                 x: Tensor # up to 2D Tensor
                ):
        """Init normalizer by storing mean and std dev"""
        self.x_mean = x.mean(axis=0)
        self.x_std = x.std(axis=0)
        
    def normalize(self,
        x: Tensor # up to 2D tensor 
                 ) -> Tensor: # x_normalized
        "Normalize (substract mean and divide by standard deviation) input tensor"
        x_mean = x.mean(axis=0)
        x_std = x.std(axis=0)

        return ((x - self.x_mean) / self.x_std)

    def reverse_normalize(self,
        x_norm, # Normalized array
                          ) -> Tensor:       # Array after reversing normalization
        return x_norm * self.x_std + self.x_mean

    def reverse_normalize_std(self,
        x_std_norm, # Normalized array of standard deviations
                          ) -> Tensor:       # Array after reversing normalization
        return x_std_norm * self.x_std

# %% ../nbs/01_Learner.ipynb 13
class GPFALearner():
    def __init__(self,
                 X: Tensor, # (n_features * n_obs) Multivariate time series
                 T: Tensor = None # (n_obs) Vector of time of observations.
                 # If none each observation is considered to be at the same distance
                ):
        self.prepare_X(X)
        if T is None: self.default_time(X)
        else: self.T = T
        
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()
        latent_kernel = gpytorch.kernels.RBFKernel()
        self.model = GPFA(self.T, self.X, self.likelihood, self.n_features, latent_kernel)
        
    @torch.no_grad()
    def prepare_X(self, X):
        self.norm = Normalizer(X)
        X = self.norm.normalize(X)
        # flatten Matrix to vector
        self.X = X.reshape(-1) 
        self.n_features = X.shape[1]
        
    @torch.no_grad()
    def default_time(self, X):
        self.T = torch.arange(X.shape[0])
        
    
    def train(self, n_iter=100, lr=0.1):
        # need to enable training mode
        self.model.train()
        self.likelihood.train()
        
        # Use the adam optimizer
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr) 
        
        self.losses = torch.zeros(n_iter)
        # "Loss" for GPs - the marginal log likelihood
        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)
        self.pb = master_bar([1])
        for _ in self.pb:
            for i in progress_bar(range(n_iter), parent=self.pb):
                # Zero gradients from previous iteration
                optimizer.zero_grad()
                # Output from model
                output = self.model(self.T)
                # Calc loss and backprop gradients
                loss = -mll(output, self.X)
                self.losses[i] = loss.detach()
                loss.backward()
                self.printer(i)

                optimizer.step()
        
        
    def printer(self, i):
        pass
        

# %% ../nbs/01_Learner.ipynb 24
@torch.no_grad() # don't calc gradients on predictions
@patch()
def predict_raw(self: GPFALearner, T):
    self.model.eval()
    self.likelihood.eval()
    return self.likelihood(self.model(T))

# %% ../nbs/01_Learner.ipynb 31
NormParam = namedtuple("NormalParameters", ["mean", "std"])

# %% ../nbs/01_Learner.ipynb 33
@torch.no_grad() # needed because raw output still has gradients attached
@patch
def prediction_from_raw(self: GPFALearner, raw_mean, raw_std):
    """ Takes a raw prediction and produces and final prediction, by reshaping and reversing normalization"""
    raw_std = raw_std.reshape(-1, self.n_features)
    raw_mean = raw_mean.reshape(-1, self.n_features)
    
    pred_mean = self.norm.reverse_normalize(raw_mean)
    pred_std = self.norm.reverse_normalize_std(raw_std)
    
    #remove pytorch gradients
    return NormParam(pred_mean.detach(), pred_std.detach())

# %% ../nbs/01_Learner.ipynb 54
def conditional_guassian(gauss: MultivariateNormal,
                         obs,
                         idx # Boolean tensor specifying for each variable is observed (True) or not (False)
                        ):
    μ = gauss.mean
    Σ = gauss.covariance_matrix
    # check idx same size of mu
    μ_x = μ[~idx]
    μ_o = μ[idx]
    
    Σ_xx = Σ[~idx,:][:, ~idx]
    Σ_xo = Σ[~idx,:][:, idx]
    Σ_ox = Σ[idx,:][:, ~idx]
    Σ_oo = Σ[idx,:][:, idx]
    
    Σ_oo_inv = torch.linalg.inv(Σ_oo)
    
    mean = μ_x + Σ_xo@Σ_oo_inv@(obs - μ_o)
    cov = Σ_xx - Σ_xo@Σ_oo_inv@Σ_ox
    
    return MultivariateNormal(mean, cov)
    

# %% ../nbs/01_Learner.ipynb 59
def _merge_raw_cond_pred(pred_raw,
                         pred_cond,
                         obs,
                         idx
                        ) -> NormParam:
    """This functions merges a complete predition with a conditional prediction and the observations.
    For the observations the std is considered to be 0 """
    mean = torch.zeros_like(pred_raw.mean) # get shape from complete prediction
    mean[~idx] = pred_cond.mean # add predictions
    mean[idx] = obs # add observations
    
    std = torch.zeros_like(pred_raw.stddev)
    std[~idx] = pred_cond.stddev
    std[idx] = 0 # there is no uncertainty as it's an oberservation
    
    return NormParam(mean, std)

# %% ../nbs/01_Learner.ipynb 65
@patch
def _normalize_obs(self: GPFALearner,
                   obs, # (n_obs)
                   idx
                  ) -> Tensor: # (n_obs)
    """ reshape the observations so they can normalized"""
    obs_compl = torch.zeros(idx.shape)
    obs_compl[idx] = obs
    obs_compl = obs_compl.reshape(-1, self.n_features)
    obs_norm = self.norm.normalize(obs_compl)
    return obs_norm.reshape(-1)[idx]

# %% ../nbs/01_Learner.ipynb 70
@patch
def predict(self: GPFALearner,
            T: Tensor, # (n_pred) time where prediction is needed
            # (n_obs_pred) Optional - if at the times of the prediction there are some observations
            # array with the values of observations to condition distribution
            obs: Tensor = None,
            # ((n_pred*n_features)) Optional - necessary if obs are present
            # Boolean array that is True where an observation is present and False where a prediction is needed
            # This is a 1D array with the length equal to n_pred (number time steps to predict) times n_features
            idx: Tensor = None
           ):
    pred_raw = self.predict_raw(T)
    
    # Conditional observations
    if obs is not None and idx is not None:
        # observations needs to be normalized before can be used with the raw prediction!
        obs_norm = self._normalize_obs(obs, idx)
        pred_cond = conditional_guassian(pred_raw, obs_norm, idx)

        pred_merge = _merge_raw_cond_pred(pred_raw, pred_cond, obs_norm, idx)
    else:
        pred_merge = NormParam(pred_raw.mean, pred_raw.stddev)
    
    return self.prediction_from_raw(pred_merge.mean, pred_merge.std)

# %% ../nbs/01_Learner.ipynb 80
def get_parameter_value(name, param, constraint):
    if constraint is not None:
        value = constraint.transform(param.data.detach())
        name = name.replace("raw_", "") # parameter is not raw anymore
    else:
        value = param.data.detach()
    return (name, value)

# %% ../nbs/01_Learner.ipynb 82
def tensor_to_first_item(tensor):
    if tensor.dim() > 0:
        return tensor_to_first_item(tensor[0])
    return tensor.item()


def format_parameter(name, value):
    value = tensor_to_first_item(value)
    name = name.split(".")[-1] # get only last part of name
    return f"{name}: {value:.3f}"

# %% ../nbs/01_Learner.ipynb 83
@patch
def get_formatted_params(self: GPFALearner):
    return ", ".join([
        format_parameter(*get_parameter_value(name, value, constraint))
        for name, value, constraint in
        self.model.named_parameters_and_constraints()
    ])

# %% ../nbs/01_Learner.ipynb 86
@patch
def printer(self: GPFALearner, i_iter):

    if i_iter%10 == 0:
        update_str = f"loss: {self.losses[i_iter].item():.3f}, " + self.get_formatted_params()
        #self.plot_loss(i_iter)
    
    #self.pb.write(update_str)
