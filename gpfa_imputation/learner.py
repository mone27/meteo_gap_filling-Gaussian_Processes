# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_Learner.ipynb.

# %% auto 0
__all__ = ['NormParam', 'GPFALearner']

# %% ../nbs/01_Learner.ipynb 4
import torch
from torch import Tensor
from torch.distributions import MultivariateNormal 

import gpytorch
from .gpfa import *
from collections import namedtuple

from fastcore.foundation import *
from fastprogress.fastprogress import progress_bar, master_bar
from fastcore.foundation import patch

# %% ../nbs/01_Learner.ipynb 15
class GPFALearner():
    def __init__(self,
                 X: Tensor, # (n_features * n_obs) Multivariate time series
                 T: Tensor = None # (n_obs) Vector of time of observations.
                 # If none each observation is considered to be at the same distance
                ):
        self.prepare_X(X)
        if T is None: self.default_time(X)
        
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()
        latent_kernel = gpytorch.kernels.RBFKernel()
        self.model = GPFA(self.T, self.X, self.likelihood, self.n_features, latent_kernel)
        
    @torch.no_grad()
    def prepare_X(self, X):
        X, self.x_mean, self.x_std = normalize(X)
        # flatten Matrix to vector
        self.X = X.reshape(-1) 
        self.n_features = X.shape[1]
        
    @torch.no_grad()
    def default_time(self, X):
        self.T = torch.arange(X.shape[0])
        
    
    def train(self, n_iter=100, lr=0.1):
        # need to enable training mode
        self.model.train()
        self.likelihood.train()
        
        # Use the adam optimizer
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr) 
        
        self.losses = torch.zeros(n_iter)
        # "Loss" for GPs - the marginal log likelihood
        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)
        self.pb = master_bar([1])
        for _ in self.pb:
            for i in progress_bar(range(n_iter), parent=self.pb):
                # Zero gradients from previous iteration
                optimizer.zero_grad()
                # Output from model
                output = self.model(self.T)
                # Calc loss and backprop gradients
                loss = -mll(output, self.X)
                self.losses[i] = loss.detach()
                loss.backward()
                self.printer(i)

                optimizer.step()
        
        
    def printer(self, i):
        pass
        

# %% ../nbs/01_Learner.ipynb 25
@torch.no_grad() # don't calc gradients on predictions
@patch()
def predict_raw(self: GPFALearner, T):
    self.model.eval()
    self.likelihood.eval()
    return self.likelihood(self.model(T))

# %% ../nbs/01_Learner.ipynb 32
NormParam = namedtuple("NormalParameters", ["mean", "std"])

# %% ../nbs/01_Learner.ipynb 34
@torch.no_grad()
@patch
def predict(self: GPFALearner, T):
    raw_out = self.predict_raw(T)
    raw_std = raw_out.stddev.reshape(0, self.n_features)
    raw_mean = raw_out.mean.reshape(-1, self.n_features)
    
    pred_mean = reverse_normalize(raw_mean, self.x_mean, self.x_std)
    pred_std = reverse_normalize_std(raw_std, self.x_std)
    # detach to avoid that gradients are calculated on results
    return NormParam(pred_mean.detach(), pred_std.detach())

# %% ../nbs/01_Learner.ipynb 57
@patch
def get_formatted_params(self: GPFALearner):
    return ", ".join([
        format_parameter(*get_parameter_value(name, value, constraint))
        for name, value, constraint in
        self.model.named_parameters_and_constraints()
    ])
