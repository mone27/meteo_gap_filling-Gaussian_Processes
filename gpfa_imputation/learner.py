# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_Learner.ipynb.

# %% auto 0
__all__ = ['NormParam', 'normalize', 'reverse_normalize', 'reverse_normalize_std', 'GPFALearner']

# %% ../nbs/01_Learner.ipynb 4
import torch
from torch import Tensor
from torch.distributions import MultivariateNormal 

import gpytorch
from .gpfa import *
from collections import namedtuple

from fastcore.foundation import *
from fastprogress.fastprogress import progress_bar, master_bar
from fastcore.foundation import patch

# %% ../nbs/01_Learner.ipynb 10
def normalize(x: Tensor # up to 2D tensor 
             ) -> tuple[Tensor, Tensor, Tensor]: # Tuple of `x_norm`, `x_mean` and `x_std`
    "Normalize (substract mean and divide by standard deviation) input tensor"
    x_mean = x.mean(axis=0)
    x_std = x.std(axis=0)

    return ((x - x_mean) / x_std), x_mean, x_std 

def reverse_normalize(x_norm, # Normalized array
                      x_mean, # mean used in normalization
                      x_std   # std dev used in normalization
                      ) -> Tensor:       # Array after reversing normalization
    return x_norm * x_std + x_mean

def reverse_normalize_std(x_std_norm, # Normalized array of standard deviations
                      x_std   # std dev used in normalization
                      ) -> Tensor:       # Array after reversing normalization
    return x_std_norm * x_std

# %% ../nbs/01_Learner.ipynb 13
class GPFALearner():
    def __init__(self,
                 X: Tensor, # (n_features * n_obs) Multivariate time series
                 T: Tensor = None # (n_obs) Vector of time of observations.
                 # If none each observation is considered to be at the same distance
                ):
        self.prepare_X(X)
        if T is None: self.default_time(X)
        else: self.T = T
        
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()
        latent_kernel = gpytorch.kernels.RBFKernel()
        self.model = GPFA(self.T, self.X, self.likelihood, self.n_features, latent_kernel)
        
    @torch.no_grad()
    def prepare_X(self, X):
        X, self.x_mean, self.x_std = normalize(X)
        # flatten Matrix to vector
        self.X = X.reshape(-1) 
        self.n_features = X.shape[1]
        
    @torch.no_grad()
    def default_time(self, X):
        self.T = torch.arange(X.shape[0])
        
    
    def train(self, n_iter=100, lr=0.1):
        # need to enable training mode
        self.model.train()
        self.likelihood.train()
        
        # Use the adam optimizer
        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr) 
        
        self.losses = torch.zeros(n_iter)
        # "Loss" for GPs - the marginal log likelihood
        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)
        self.pb = master_bar([1])
        for _ in self.pb:
            for i in progress_bar(range(n_iter), parent=self.pb):
                # Zero gradients from previous iteration
                optimizer.zero_grad()
                # Output from model
                output = self.model(self.T)
                # Calc loss and backprop gradients
                loss = -mll(output, self.X)
                self.losses[i] = loss.detach()
                loss.backward()
                self.printer(i)

                optimizer.step()
        
        
    def printer(self, i):
        pass
        

# %% ../nbs/01_Learner.ipynb 24
@torch.no_grad() # don't calc gradients on predictions
@patch()
def predict_raw(self: GPFALearner, T):
    self.model.eval()
    self.likelihood.eval()
    return self.likelihood(self.model(T))

# %% ../nbs/01_Learner.ipynb 31
NormParam = namedtuple("NormalParameters", ["mean", "std"])

# %% ../nbs/01_Learner.ipynb 33
@torch.no_grad()
@patch
def predict(self: GPFALearner, T):
    raw_out = self.predict_raw(T)
    raw_std = raw_out.stddev.reshape(-1, self.n_features)
    raw_mean = raw_out.mean.reshape(-1, self.n_features)
    
    pred_mean = reverse_normalize(raw_mean, self.x_mean, self.x_std)
    pred_std = reverse_normalize_std(raw_std, self.x_std)
    # detach to avoid that gradients are calculated on results
    return NormParam(pred_mean.detach(), pred_std.detach())

# %% ../nbs/01_Learner.ipynb 56
@patch
def get_formatted_params(self: GPFALearner):
    return ", ".join([
        format_parameter(*get_parameter_value(name, value, constraint))
        for name, value, constraint in
        self.model.named_parameters_and_constraints()
    ])
